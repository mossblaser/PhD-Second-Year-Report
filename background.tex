\chapter{Background}
	
	\label{sec:background}
	
	This chapter aims to familiarise the reader with the principles of neural
	modelling and the challenges involved in simulating these models at biological
	scales and speeds. An overview is given of the current state-of-the-art of
	supercomputer technologies and their suitability for neural modelling. This is
	followed by an introduction to specialised `neuromorphic' systems which
	specifically target neural simulation tasks and overcome the limitations of
	current supercomputers. Finally, the chapter covers the interconnection
	network of SpiNNaker, a neuromorphic platform which will form a platform for
	the work proposed later in this report.
	
	\section{Modelling the brain}
		
		% What is done, for what purpose
		
		The brain is an extremely complex and variable organ whose high-level
		behaviour is similarly complex and variable. Unsurprisingly, efforts to
		understand the brain rely heavily on simplified models of its function and
		implementation. Modelling approaches cover a wide spectrum from extremely
		detailed models of the neurophysiology of individual neurons
		\cite{carnevale06} to high level models where mathematical functions are
		mapped automatically onto large populations of neurons \cite{eliasmith04}.
		This report focuses towards the latter end of the spectrum. This section
		gives a brief historical context for the modelling approaches in use today
		followed by an overview of the NEF, a high-level modelling framework to give
		a feel for the way networks are built.
		
		\subsection{History of neural modelling}
			
			The development of high-level neural models can be divided up into three
			coarse generations, each increasing their level of biological realism
			\cite{vainbrand11}.
			
			The first generation of models, such as the McCulloch-Pitts threshold
			neuron \cite{mcculloch43}, consisted of testing if a simple, linear
			function of the neuron's inputs was above a threshold value and outputting
			either a `high' or `low' signal. The function used in each neuron and the
			pattern of connectivity in the network defines the behaviour of the
			network as a whole.
			
			It was noted that communication between biological neurons was not simply
			level-based but instead appeared to be based on the rate at which
			electrical `spikes' were emitted (or `fired') by neurons to their
			neighbours. The second generation sought to model this with neurons which
			produced a continuous (analogue) value which represented the firing rate
			\cite{maass97}. Once again, the network's behaviour was defined by the
			functions computed by each neuron and the network's connectivity.
			
			\begin{figure}
				\center
				\input{|"python2 figures/snn-example.py"}
				\caption{Simple leaky-integrate-and-fire neuron example.}
				\label{fig:snn-example}
			\end{figure}
			
			The third generation of extended the idea further noting that the firing
			rate is not the only significant factor but also that the timing of spikes
			is significant too \cite{maass01}. As a result, neural models were devised
			that modelled some of the physiological properties of neurons.
			
			The level of biological detail of such models varies greatly but many
			models build on the `leaky integrate and fire' (LIF) model.  Spikes
			arriving at a LIF neuron cause a temporary flow of current into or out of
			the neuron, modelling the behaviour of synapses in biological neurons. The
			LIF neuron integrates this current over time, accumulating charge which
			gradually leaks away. If the charge in the neuron reaches a certain
			threshold, the neuron produces a spike and its charge is reset. This
			process is illustrated in figure \ref{fig:snn-example}.
			
			One of the key parameters of a neural network is the amount of influence
			each incoming spike has on a neuron. Typically, this influence is modelled
			by assigning a `weight' to each synapse which scales the impact of a spike
			arriving via that synapse.
			
			Models of many types of learning revolve around modelling changes in
			weights over long periods of time observed within the brain. The exact
			rules by which these weights are adjusted is the subject of much active
			research though most promising approaches attempt to learn from the
			relative timing \cite{pfister06} or rate \cite{bienenstock82} of spikes
			arriving at a neuron. As well as adjusting weights, some learning rules
			can also form entirely new connections between previously unconnected
			neurons \cite{bamford10}.
		
		\subsection{High-level modelling}
			
			The Neural Engineering Framework (NEF) \cite{eliasmith04} is a high level
			framework for constructing large neural models which exhibit complex,
			high-level behaviour. The NEF has been used to build large, complex models
			including Spaun as described in \emph{`How to build a brain'}
			\cite{eliasmith13}. The key to the NEF's ability to model complex
			behaviour is its high-level approach to neural network design. This
			high-level approach is based on three `principles' which are enumerated
			below:
			
			\begin{description}
				
				\item[Representation] The spiking activity of a population of simple
				neuron models encode vectors (i.e. an array of numerical values).
				
				\item[Transformation] Arbitrary mathematical functions can be computed
				by selecting appropriate connection weights between populations of
				neurons representing input values and a target population which will
				represent the result.
				
				\item[Dyanmics] By connecting populations of neurons back to themselves,
				a control system is formed which can be tuned to produce interesting
				dynamical behaviour. This can be used, for example, as a basis for
				working memory.
				
			\end{description}
			
			By applying these principles, the NEF can be considered analogous to a
			compiler for neural models. An implementation of the NEF for the Python
			programming language called Nengo has been produced which enables users to
			describe functions in standard Python code which are then directly
			`compiled' into a neural model. Using this system, `libraries' of useful
			neural models and subcomponents have been built. A prominent example is
			the Semantic Pointer Architecture (SPA) \cite{eliasmith13} which provides
			additional higher level features including a symbolic representation
			system.
		
	
	\section{Simulating brains}
		
		The range of neural models described in the previous section all present a
		significant computational challenge to simulate at scale. This section
		briefly outlines the challenges faced when simulating large models.
		
		% TODO Introduce
		
		\subsection{Computation}
			
			Computationally, spiking neural models can be relatively simple, the state
			of the neuron is described by a differential equation over time. Neuron
			states are typically updated with a fairly coarse granularity of once per
			0.1 to 1.0 ms. Additional calculations are also required to update the
			neurons' states upon spike arrival. In the case of networks featuring
			learning models, spike arrivals also typically entail further calculations
			to update weights.
			
			Given that the human brain contains around 100 billion ($10^9$) neurons
			each with around 10,000 synapses, this constitutes a non-trivial amount of
			computation. To maintain realtime performance, the simulation of different
			neurons must be distributed across a large number of processing cores.
		
		\subsection{Communication}
			
			% Number of spikes produced, the form that spikes take, multicast,
			% possible changes due to learning. Especially fun for realtime.
			
			% spiking at an average rate of 10 Hz
			
			The feature of neural models which stresses simulators most, however, is
			their communications requirements. Since neurons in the brain fire at an
			average rate of around 10 Hz in a human-sized model, 100 trillion
			($10^{14}$) spikes must be delivered within a model every second. Since
			neurons connect to around 10,000 others, their spikes need to be
			transmitted to a large number of destinations. Unfortunately, conventional
			computer networks tend to be focused on supporting efficient one-to-one
			(unicast) connectivity. In many cases meaning that each spike must be
			repeatedly transmitted, once for each destination, which costs both large
			amounts of network resource and consequently energy. To achieve greater
			efficiency, `multicast' communications can be used where messages are sent
			once and subsequently delivered to multiple locations, only being
			duplicated within the network at the latest possible moment.
			
			An additional challenge for interconnection models is the amount of data
			involved in communications within neural simulators. Spike messages encode
			only a very limited amount of data: their source neuron and the time they
			were produced. This means that messages sent through the network tend to
			be very small even though the aggregate network utilisation may be high.
			This pattern is directly opposed to many conventional computational
			problems which instead tend to transmit large, continuous blocks of data
			at more infrequent intervals. As a result, conventional networks can
			impose large overheads when dealing with spikes due to their small size.
			Some researchers have experimented with sending many spikes at once to
			reduce the overhead involved however the effectiveness of this technique
			can be limited in certain models \cite{morrison05}.
			
			Luckily, much of the connectivity within the brain is highly local,
			meaning that neurons tend to connect to nearby neurons. This type of
			predominantly local communication is well supported by most scalable
			computer networks such as those found in supercomputers. However, to take
			advantage of this property models must be carefully laid out within a
			simulator's network. This problem is known to be NP-complete though
			effective heuristics solutions exist \cite{haldar00}.
		
		
	
	\section{Supercomputer technology}
		\label{sec:supercomputers}
		
		\begin{table}
			\center
			\begin{tabular}{r l r r r l l l}
				\toprule
				Rank & Name    & Pflops& Cores  & Nodes  & Topology & Interconnect          & Sources \\
				\midrule                          
				1 & Tianhe-2   & 33.86 & 3,120,000 & 16,000 & Fat-Tree & Electrical \& Optical & \cite{dongarra13} \\
				2 & Titan      & 17.59 & 560,640   & 18,688 & 3D Torus & Electrical            & \cite{bland12} \\
				3 & Sequoia    & 17.17 & 1,572,864 & 98,304 & 5D Torus & Electrical \& Optical & \cite{prickett10} \\
				4 & K Computer & 10.51 & 705,024   & 68,544 & 6D Torus & Electrical            & \cite{fujitsu11,yokokawa11} \\
				5 & Mira       &  8.59 & 786,432   & 49,152 & 5D Torus & Electrical \& Optical & \cite{prickett10} \\
				\bottomrule
			\end{tabular}
			
			\caption{Top Five `Top500' Supercomputers, June 2014.}
			\label{tab:top500}
		\end{table}
		
		The Top500 list \cite{meuer14j} aims to enumerate, biannually, the 500
		fastest supercomputers in the world ranked by their performance on the
		LINPACK benchmark \cite{dongarraLINPAC}. The list offers an insight into the
		current state-of-the-art for high-performance computing. Many of the Top500
		supercomputers are designed to cope with detailed physical simulations for
		applications such as physics simulations, climate modelling, medical
		treatment design and nuclear energy research
		\cite{olcf12,nassif,dongarra13}.
		
		Table \ref{tab:top500} shows the top five machines in the Top500 list
		released in June 2014 along with basic details of the type of
		interconnection involved.  In this section an overview is given of the
		architecture of these large scale machines.
		
		The LINPACK benchmark performs computations to ``analyze and solve linear
		equations and linear least-squares problems'' to produce a computational
		load representative of certain computational tasks common to scientific
		computing \cite{dongarra84}. In particular it is a CPU-bound problem which
		attempts to measure the peak CPU performance achievable\footnote{In LINPACK,
		CPU Performance is measured in Petaflops: how many quadrillion ($10^{15}$)
		floating point operations can be performed per second.} but without any
		significant indication of the performance of the network which connects the
		system together \cite{dongarra07}.
		
		Compared with LINPACK, simulation of spiking neural networks requires far
		more communication resource but relatively small amounts of computation
		making it a poor benchmark if results are taken at face-value. In fact, many
		modern supercomputer workloads also rely on network performance to a greater
		degree than the LINPACK benchmarks and as a result a new ranking, Graph500
		\cite{murphy13n}, has appeared to address this shortcoming. This
		complementary ranking instead uses benchmarks based on graph traversal
		problems \cite{murphy10}. Such problems rely on having efficient
		point-to-point communication between different parts of the system where
		each section of the graph resides. Since there is a high degree of
		correlation between the rankings of the two lists, only the more
		comprehensive Top500 list is considered here.
		
		
		\subsection{Computation}
			
			Supercomputers are typically built by combining a large number of
			`processing elements' in such a way that they are able to perform a single
			task coherently. The processing elements used can come in many forms
			though the three most prevalent are:
			
			\begin{description}
				
				\item[General Purpose Processor] A conventional processor `core', the
				most common amongst the Top500.
				
				\item[Graphics Processing Unit (GPU)] A specialised vector processor
				able to perform a single operation on many data simultaneously. GPUs are
				growing in popularity in the Top500 and are used extensively in
				high-ranking machines such as Titan \cite{bland12}.
				
				\item[Accelerators] Special purpose devices, sometimes application
				specific. Examples include the Xeon Phi accelerator used in Tianhe-2
				\cite{dongarra13} and Field Programmable Gate Arrays (FPGAs) which can
				be programmed to implement specialised algorithms in hardware.
				
			\end{description}
			
			A number of these processing elements are then combined to create a single
			`node' in the system. This may take the form of a chip or a collection of
			components sharing a circuit board. Typically the elements within a node
			are able to communicate relatively cheaply while messages to remote nodes
			must traverse a slower, system-wide interconnection network.
		
		\subsection{Interconnect}
			
			In this subsection, the general structure and function of supercomputer
			interconnection networks is described, beginning with details of the
			high-level topology of the system and moving towards the low-level
			functions of the routers and link technologies used.
			
			\subsubsection{Topology}
				
				% Topology has influence on structure of the system, supercomputers are
				% optimised for local access of data near by. Two topologies are
				% popular: trees and tori.
				%
				% Trees have low hop counts and expensive routers, tori have higher hop
				% counts and cheaper routers. Both are easily partitioned.
				
				The topology of a supercomputer's interconnection network dictates the
				physical connections required to build it and constrains the patterns of
				communication the system can support. Topologies are typically designed
				such that communication between nearby nodes is cheap and does not
				compete with distant nodes for communication resources. The top five
				machines in the Top500 list achieve this using the `fat tree' and
				`torus' topologies which are described below:
				
				\begin{figure}
					\begin{subfigure}[t]{\textwidth}
						\center
						\input{figures/fat-tree-concept}
						\caption{Basic fat tree links.}
						\label{fig:fat-tree-concept}
					\end{subfigure}
					
					\vspace{1.5em}
					
					\begin{subfigure}[t]{\textwidth}
						\center
						\input{figures/fat-tree-clos}
						\caption{Folded Clos network.}
						\label{fig:fat-tree-clos}
					\end{subfigure}
					
					\caption[Fat tree topologies.]{Fat tree topologies. Thicker lines
					represent higher bandwidth links.}
					\label{fig:fat-tree}
				\end{figure}
				
				\label{sec:fat-tree}
				
				In a basic fat tree, nodes are connected as leaves in a tree structure
				of multiple layers of switches (figure \ref{fig:fat-tree-concept}).
				Connections higher in the hierarchy are connected via links of
				increasing bandwidth to avoid bottle-necks.  Nodes communicate by
				sending messages up the tree until they reach a node of which the
				destination as a child at which point the message is forwarded down the
				tree towards its destination.
				
				In practice it is not possible to build switches with sufficient
				bandwidth to act as a root node to most supercomputer networks. In
				addition, the root switch is a potential single point of failure for the
				whole system. As a result, folded Clos networks \cite{clos53} are often used instead
				(figure \ref{fig:fat-tree-clos}), as in `Tianhe-2', and have become
				synonymous with the fat tree topology. Here, higher levels of the
				hierarchy are duplicated introducing redundancy and spreading the load.
				
				Fat trees support small maximum `hop' counts; that is the number of hops
				a message has to make between nodes in the network. Average hop counts
				grow in fat trees by $O(\log{N})$ with respect to the number of nodes in
				the system. Additionally, they support cheap local communication between
				nodes nearby in the tree.
				
				Unfortunately, tree-based networks depend on complex, high-radix
				switches to connect many nodes simultaneously.  Tianhe-2, for example,
				has thirteen 576-port switches based on a custom router chip. Such
				architectures often prove difficult to extend once implemented
				\cite{dally04}.
				
				\begin{figure}
					\begin{subfigure}[t]{\textwidth}
						\center
						\input{figures/torus-flat}
						\caption{Mesh (Grey lines show wrap-around connections added in a
						torus)}
						\label{fig:torus-flat}
					\end{subfigure}
					
					\vspace{1em}
					
					\begin{subfigure}[t]{\textwidth}
						\center
						\input{figures/torus-pipe}
						\caption{Rolled into a tube}
						\label{fig:torus-pipe}
					\end{subfigure}
					
					\vspace{1em}
					
					\begin{subfigure}[t]{\textwidth}
						\center
						\input{figures/torus-3D}
						\caption{Bent into a torus}
						\label{fig:torus-3D}
					\end{subfigure}
					
					\caption{Transformation of a mesh into a torus.}
					\label{fig:forming-a-torus}
				\end{figure}
			
				The most common topology in the top-five is the torus (also known as a
				$k$-ary $n$-cube). Here, nodes are arranged in a $n$-dimensional mesh.
				Nodes at the extreme edges of the mesh are connected together to form a
				torus. For the 2D case figure \ref{fig:forming-a-torus} shows how a
				mesh, when rolled up (figure \ref{fig:torus-flat}) and bent into a
				doughnut or torus shape (figure \ref{fig:torus-3D}), giving network its
				name.
				
				Each node is able to communicate directly with its immediate neighbours
				in each dimension, that is above, below, left and right in the 2D case.
				More distant nodes are able to communicate by forwarding messages via
				intermediate nodes.
				
				As in the fat tree, nearby nodes are cheap to reach however a greater
				number of hops is required in the worst case: in a torus $k$ nodes long
				in each of $n$ dimensions the worst case path length is $\frac{kn}{2}$
				\cite{dally04}.
				
				Switches in torus networks can be simpler meaning that the worst case
				performance of both types of network can be comparable (17 $\mu$s for a
				broadcast in Tianhe-2 and 9 $\mu$s in Sequoia
				\cite{dongarra13,morozov12}).
				
				The differences in performance between tori and trees depends strongly
				on the application, however. Work by Vainbrand and Ginosar
				\cite{vainbrand11} showed that for spiking neural networks torus
				networks are preferable to fat-trees.
			
			\subsubsection{Link technology}
				
				Interconnect in modern supercomputers is made up almost exclusively of
				High-Speed Serial (HSS) links (see \S\ref{sec:high-speed-serial})
				running over either electrical or optical connections. Electrical
				transmission technologies are generally cost less money than optical
				systems for short distances and lower bandwidths. As a result
				connections between physically neighbouring nodes are almost universally
				connected via such links.  Optical links, however, are favoured by
				supercomputer designs between cabinets containing many individual nodes
				in systems such as Blue Gene/Q and Tianhe-2
				\cite{dongarra13,prickett10}.  These optical links are typically used to
				carry the equivalent of many electrical signals and side-step
				difficulties with driving long distance electrical connections.
				
				HSS links typically consume the same power when idle as when fully
				loaded.  Power-efficient use of the link therefore requires implies that
				either the link is heavily loaded or that traffic in the network
				tolerate long latencies of thousands of cycles as links are powered-on
				and off in response to load \cite{soteriou03}.  For neural simulations,
				which are generally latency sensitive and produce difficult to predict
				network traffic depending both on the model's state and the model
				itself, this will require careful tuning of link parameters to achieve
				power good efficiency.
				
				% TODO: Describe power control options for neural things
			
			\subsubsection{Packet formatting}
				
				Supercomputer networks are generally designed to handle the transfer of
				large bursts of data -- on the order of several kilobytes -- in length
				making use of technologies such as Message Passing Interface (MPI)
				\cite{mpiforum12}. Since every MPI message must contain a header
				describing, for example, the message's origin and purpose, longer
				messages keep the overall overhead introduced low.
				
				This pattern is also repeated in the low-level link protocols used by
				supercomputers.  For example, in the widely used InfiniBand high-speed
				serial interconnect technology, 120 bytes of metadata are added to each
				packet \cite{infinibandta08}. This metadata allows the system to perform
				important tasks such testing for, and retransmitting, corrupt packets,
				routing packets to their correct destination and applying flow control.
				Each packet can contain up to 4,096 bytes of payload meaning the
				protocol overhead can be as low as 2.8\%.
				
				This pattern of communication matches many supercomputer applications
				as is reflected by the prominence of array and matrix functions in
				parallel programming APIs such as MPI. Unfortunately, this is a poor
				match for neural simulations which may transmit only a single 32-bit
				integer to communicate spikes \cite{davies12}. If such a spike were
				transmitted individually over an InfiniBand link, the metadata would
				make up 96.8\% of the packet wasting significant bandwidth and energy.
				
				% TODO: Talk about work done by Xilinx and others. (Maybe?)
	
	
	\section{Neuromorphic computing}
		
		So-called `Neuromorphic' systems aim to reproduce neuro-biological phenomena
		using specialised electronic circuits \cite{mead90}. Neuromorphic computing
		devices are generally specialised computers which allow time and power
		efficient modelling of networks of neurons. While neuromorphic devices vary
		widely in their construction, scale, accuracy and flexibility this section
		aims to briefly survey a representative selection of current approaches.
		
		This section divides its attention between neuromorphic systems which,
		unconventionally, use analogue components to accelerate computation and
		those which take a purely digital approach. This section concludes with an
		overview of the interconnection network of SpiNNaker, the machine which
		forms the platform on which this work builds.
		
		\subsection{Analogue and mixed-mode}
			
			Though analogue technology has long been out of favour for general purpose
			computing, neural simulation lends itself to analogue technology.  Digital
			systems offer exact, repeatable computation and communication at the
			expense of power. The brain, and many neural models, however, do not
			require such precision and here analogue approaches offer the potential
			for both power, area and performance improvements. For example, a leaky
			integrator can be implemented using just a capacitor and resistor while
			the equivalent digital circuit would use tens of transistors
			\cite{misra10}.
			
			A wide range of techniques for implementing neural models in analogue
			circuitry has been proposed \cite{graf86,holler89,agranat90,azghadi13}.
			Unfortunately although the lack of precision of analogue circuits is not a
			problem for neural modelling, the lack of consistency is. The same
			analogue circuit may have widely different characteristics in one part of
			a chip compared to another due to variations in the silicon wafer. Such
			circuits must be calibrated to allow meaningful simulations to take place.
			In addition, analogue circuits must tolerate changes in temperature and
			voltage without affecting the model's behaviour.
			
			So-called `mixed mode' systems have been designed such as BrainScaleS and
			Neurogrid, described below, which combine analogue computational
			components with digital communication \cite{maguire07,benjamin14}. The use
			of digital communication avoids issues with inconsistencies in
			interconnect circuitry across the chip while also simplifying routing of
			spikes which can then share a limited number of wires using conventional
			digital multiplexing methods, rather than one-wire-per-connection as in
			biology.
			
			\subsubsection{BrainScaleS}
				
				% TODO: Include photograph
				\begin{figure}
					\center
					\includegraphics[width=0.5\textwidth]{figures/brainscales}
					
					\caption[The BrainScaleS wafer-scale system.]{The BrainScaleS
					wafer-scale system (right) alongside its supporting conventional PC cluster.}
					
					\label{fig:brainscales}
				\end{figure}
				
				The BrainScaleS project makes use of wafer-scale integration where an
				entire silicon wafer, usually used to manufacture hundreds of individual
				chips, is used as a single system. Instead of slicing the wafer into
				individual dice, space between the reticles is used to build an
				interconnection network to yield a single large system. A system built
				from a single wafer is able to simulate a network of around 200,000
				neurons up to $10^5$ times faster than biological realtime while using
				6,000 times less power than a conventional supercomputer
				\cite{schemmel08}.
				
				Each reticle in the wafer contains eight Analog Neural-Network Chips
				(ANCs) which can be programmed to model up to 254 neurons each
				\cite{schemmel10}. There are two types of interconnect between ANCs in
				the system named `level 1' (L1) and `level 2' (L2) which are described
				below.
			
				The L1 network is a mesh network connecting all the ANCs on the same
				wafer \cite{fieres08}. The network is circuit-switched, meaning that
				connecting a set of points in the mesh `uses up' wires along that path
				preventing others from using them. Circuit switched networks offer low
				latency and low-power connections between ANCs compared with a packet
				switched network (more commonly found in modern networks). This
				performance comes at the expense of flexibility as lanes, once
				allocated, cannot be used for any other purpose even when idle. This can
				make changing the connectivity within the network at runtime difficult
				\cite{dally04}, a task required by some learning rules.
				
				Since the L1 network is limited to a single wafer, the L2 network aims
				to work around this limitation. The L2 network is based on 10 gigabit/s
				Ethernet with packets being routed between wafers using off-the-shelf
				network switches and custom FPGA hardware \cite{schemmel10}. This
				network suffers higher latencies than the L1 network but allows signals
				to be routed much more flexibly enabling expansion to multiple wafers
				and other peripherals.  Ethernet (as found in supercomputer networks) is
				also largely optimised for transmission of larger chunks of data.
			
			\subsubsection{Neurogrid}
				
				\begin{figure}
				\end{figure}
				
				\begin{figure}
					\begin{subfigure}[b]{0.49\textwidth}
						\center
						\includegraphics[width=\textwidth]{figures/neurogrid}
						\vspace{0.2cm}
						
						\caption{A 16 chip Nerogrid system.}
						\label{fig:neurogrid}
					\end{subfigure}
					\begin{subfigure}[b]{0.49\textwidth}
						\center
						\input{figures/neurogrid-topology}
						\caption{Neurocores in a 16-chip Neurogrid system.}
						\label{fig:neurogrid-topology}
					\end{subfigure}
					
					\caption{Overview of the Neurogrid architecture.}
					\label{fig:neurogrid-arch-overview}
				\end{figure}
				
				The Neurogrid architecture consists of a tree of 16 `Neurocore' chips.
				Each Neurocore contains a grid of $256 \times 256$ analogue neuron
				models with a modest number of tunable parameters and which run at
				biological realtime. The full 16-Neurocore system can simulate around 1
				million neurons using around three watts \cite{benjamin14}.
				
				When spikes are produced these are transmitted via a digital network to
				all connected neurons\footnote{Neurogrid also features limited, tunable
				analogue connectivity for special, local connectivity cases which is not
				discussed here.}. Spikes produced by or destined for a Neurocore are
				streamed one by one to or from the board respectively
				\cite{boahen04,boahen04receiver}.
				
				Neurogrid's analogue neuron models do not account for weighted
				connections so an FPGA is also attached to the network which can be
				configured to randomly drop spikes sent via it to a given neuron. By
				altering the probability of spike dropping variable connection strengths
				can be approximated for networks where spike rates, but not spike
				timing, are significant. The Neurogrid tree network also includes a USB
				connection for loading neuron parameters and reading results. 
				
				The Neurocore router and its interconnect, described in detail by
				Merolla \emph{et. al.} \cite{merolla14}, is implemented entirely in
				asynchronous logic. Routers communicate on- and off-chip via 1-of-4
				non-return-to-zero links where two electrical transitions are
				required to transmit and acknowledge pairs of two bits. This scheme
				ensures that, while idle, the router consumes very little power and also
				minimises unnecessary electromagnetic noise which may disrupt sensitive
				analogue neuron circuitry on the chip.
				
				Neurogrid uses multicast wormhole routing allowing the network to
				support both large packets during configuration and small packets for
				spike transmission. Wormhole routing allows routers to begin forwarding
				packets immediately rather than storing and then forwarding the whole
				packet. This reduces buffering requirements but also makes it possible
				for the network to deadlock since individual packets can use resources
				in many chips at once and introduce cyclic dependencies \cite{dally04}.
				To avoid deadlock, Neurogrid uses a tree topology as illustrated in
				figure \ref{fig:neurogrid-topology}.  Trees are provably deadlock free
				for unicast traffic and also multicast traffic routed in a specific way.
				
				Since Neurogrid does not use a fat tree topology (described in
				\S\ref{sec:fat-tree}), the network has a throughput bottleneck at the
				root Neurocore's links. The severity of this bottleneck is greatly
				reduced, however, due to the multicast nature of the system. In the
				worst case where all $N$ neurons are connected to every other neuron,
				the throughput requirements of the root node drop from $N^2$ using
				unicast to $N$ for multicast. While this makes the system practical at a
				size of up to 16 Neurocores, if the number of chips was increased, this
				bottleneck would force the use of either higher speed links throughout
				or a change in topology.
			
			
		\subsection{Digital}
			
			% Use digital implementations of neurons, typically optimised in some
			% fashion.
			
			To avoid the difficulties which plague analogue systems, a number of
			researchers have constructed special purpose digital neuron models
			\cite{prange93,jahnke96,schoenauer99,mehrtash03}. Though in principle more
			power efficient and faster than similar software models, these designs are
			inherently tied to a single neural model.
			
			Continued gains have been made in power efficiency and speed made by
			general purpose digital platforms such as CPUs, GPUs and FPGAs.
			Consequently, the current generation of digital simulators is dominated by
			these more flexible platforms. The mature FPGA-based Bluehive and
			CPU-based SpiNNaker platforms are described below to give an overview of
			the state of current digital simulator technology.
			
			\subsubsection{Bluehive}
				
				% TODO: Include photograph
				\begin{figure} \center
				\includegraphics[width=0.4\textwidth]{figures/bluehive}
					
					\caption{A 16 FPGA Bluehive system.} \label{fig:bluehive} \end{figure}
				
				The Bluehive system connects up to 64 high-end Altera Stratix IV FPGAs,
				each simulating up to 64,000 neurons with 1,000 synapses each
				\cite{moore12}. Unlike the analogue systems (and a number of the fixed
				digital systems) which simulate all their neurons in parallel, Bluehive
				multiplexes them four-at-a-time taking advantage of high speed FPGA
				logic. Since duplicating a neuron model will quickly fill an FPGA, it is
				advantageous to split processing and storage, using the FPGA for
				processing and off-the-shelf memories for storage. In this manner, more
				attention can be spent on making the neuron model fast than making it
				compact. In analogue models, this approach would be impractical because
				analogue state is difficult to save and restore.
				
				The neuron models have been implemented both as custom, special purpose
				hardware and as conventional C code for a specialised vector processor
				(BlueVec) implemented within the FPGA logic. For the same neuron model,
				a C (software) implementation running on BlueVec required an order of
				magnitude less development effort than the custom hardware
				implementation despite only a 50\% reduction in performance.
				
				The FPGAs are interconnected in a 3D torus ($k$-ary 3-cube) using
				electrical high-speed serial links. A custom protocol is used which
				allows spikes to be transmitted reliably to multiple FPGAs,
				automatically making use alternative connections if a cable is damaged
				or unplugged. The protocol does not, however, include the ability to
				transmit packets to several destinations. As a result, a specialised
				`fan-out engine' is used which repeatedly transmits a spike once for
				each destination.
				
				Each of the six bidirectional links are operated at 10 Gbit/s per
				direction and remain powered on to ensure low latency
				communication between FPGAs. In practice, each is only loaded on average
				with about 250 MBit/s of traffic which helps reduce congestion in the
				network and thus enable lower latencies.
				
				% TODO: Awaiting some details from Theo about the network.
			
			\subsubsection{SpiNNaker}
				
				% CPU based, custom asynchronous multicast interconnect but scaling up
				% with high-speed serial 2D hexagonal torus network.
				
				% TODO: Refer to this in text.
				\begin{figure}
					\input{figures/spinnaker-abstractions}
					
					\caption{SpiNNaker hardware abstractions.}
					\label{fig:spinnaker-abstractions}
				\end{figure}
				
				The SpiNNaker architecture consists of a network of up to one million
				low-power mobile-phone grade ARM processors principally interconnected
				using a custom asynchronous network \cite{furber06}. The use of general
				purpose processors allows a great degree of flexibility in the neuron
				implementation at the cost of energy efficiency. 
				
				Figure \ref{fig:spinnaker-abstractions} shows the levels of hardware
				abstraction in SpiNNaker. As in Bluehive, each processor core in
				SpiNNaker is responsible for a number of neurons. The design targets up
				to 1,000 neurons per core each with 1,000 synapses. Eighteen ARM cores
				are combined onto a single SpiNNaker chip which are grouped into boards
				which are assembled into cabinets which will eventually make up a
				machine with 1,036,800 cores capable of simulating around 1 billion
				neurons in biological realtime.  At the time of writing, a
				$\frac{1}{10}^{\textrm{th}}$ scale system is being assembled with
				103,680 cores with boards for a full scale system currently being
				manufactured and tested.
				
				\begin{figure}
					\center
					\input{figures/spinn-topology}
					
					\caption[2D hexagonal torus topology used in SpiNNaker.]{2D hexagonal
					torus topology used in SpiNNaker. Each box is an eighteen core
					SpiNNaker chip. Links at the edges of the network wrap-around to
					connect to those on the other side. Actual networks are much larger.}
					\label{fig:spinn-topology}
				\end{figure}
				
				Chips are connected in a 2D hexagonal torus topology, illustrated in
				figure \ref{fig:spinn-topology}, using a combination of custom
				asynchronous interconnect between chips on the same circuit board (as in
				Neurogrid) and high-speed serial between boards (as in Bluehive).
				
				Unlike Neurogrid and Bluehive, the SpiNNaker network is potentially
				vulnerable to deadlock when heavily congested. To reduce the risk of
				this occurring, the network is over-provisioned such that during
				expected operation it should be lightly loaded. Additionally, if a
				deadlock occurs, it is broken using a simple packet dropping mechanism
				when a packet has taken too long to progress. Though this means packet
				delivery is not guaranteed, it is assumed that likelihood of this
				occurring is low and that neural networks are inherently tolerant to the
				rare loss of spikes, as observed in biology. For networks where
				guaranteed delivery is desired, dropped packets are not actually lost,
				instead they're copied into a buffer where user software is able to
				reinject them at an appropriate moment.
	
	\section{SpiNNaker network architecture}
		\label{sec:spinnaker}
		
		% Greater detail intro to SpiNNaker since it will be the focus of this work
		%
		% Overview of system: cores, chips, boards, racks, cabinets. Network is
		% hexagonal torus with nodes being chips.
		
		This section gives a more detailed overview of SpiNNaker's network
		architecture which forms the focus of current preliminary work and a
		platform for future work. In particular, the section outlines the format of
		packets within the network and the methods by which they are routed. The
		section concludes with a description of the link technology from which the
		network is implemented.
		
		\subsection{SpiNNaker packets}
			
			% Packet types and sizes. Table based routing, generally, multicast
			% routing in SpiNNaker. Also describe the sort of fun which can be had
			% with multicast, mention power savings due to less hops. Mention router
			% simplicity, limitations, assumptions.
			
			\begin{figure}
				\center
				\input{figures/spinnaker-packet}
				
				\caption{SpiNNaker multicast packet format.}
				\label{fig:spinnaker-packet}
			\end{figure}
			
			Cores within SpiNNaker are able to communicate through the transmission of
			small `SpiNNaker packets'. These packets are either 40 or 72 bits long and
			are designed primarily for the efficient communication of neural spikes.
			Though a number of types of packet exist for system configuration
			purposes, applications typically communicate using multicast packets which
			are formatted as shown in figure \ref{fig:spinnaker-packet}. The `control'
			field essentially indicates the type of packet. The `key' (interchangeably
			referred to as `routing key') field is specified by the application and is
			used to route the packet through the system. Finally, an optional
			`payload' field can be added by applications that require it.
			
			In typical spiking neural applications the payload field isn't required
			meaning that all packets sent are only 40 bits long. A neural spike event
			contains only two pieces of information: the identity of the neuron that
			fired and the time that it occurred. Since SpiNNaker operates in realtime
			and the network is designed to deliver packets within a single simulation
			time-step, the time of the spike is implied by the time of the packet's
			arrival. This simply leaves the neuron's identity which is also used as a
			routing key \cite{davies12}.
		
		\subsection{Routing}
			
			\label{sec:spinn-router}
			
			One of the key features of SpiNNaker's network is its support for
			multicast routing of packets. Packets are forwarded through the network by
			the router included in each eighteen core SpiNNaker chip. When a multicast
			packet arrives at a router, it looks up the key in a routing table to
			select a set of destinations for the packet. The router can forward
			packets to any combination of routers on neighbouring chips and to each of
			the eighteen local cores. By configuring the routing tables appropriately,
			spikes produced by the simulated neurons can be forwarded and multicast to
			every core in which they're required.
			
			Since the system is designed to deal with up to a billion neurons, the
			number of routing table entries required to route every key would be
			prohibitive. To reduce the number of routing entries required, a number of
			simple compression techniques are required.
			
			The first compression technique exploits the fact that in most neural
			simulations neurons are grouped into populations. Spikes from neurons in a
			particular population can typically all routed in the same way. To exploit
			this, routing keys may be split into two parts: one of which identifies
			the population from which a spike originates and another which identifies
			the neuron within that population.  By configuring the router to lookup
			the key using only the part which indicates the population, a single
			routing entry can be used for an entire population. Since populations may
			contain thousands of neurons, this technique substantially reduces the
			number of entries required.
			
			To implement this first compression technique, each entry in the routing
			table contains a key, mask and destination field. The mask field can be
			used to elide bits in a packet's key, for example those which correspond
			to specific neurons within a population. If the packet's masked key
			matches the key in the table entry, it is routed according to the
			destination field. Using a hardware device called a ternary Content
			Addressable Memory (CAM), this masking and comparison operation can be
			performed for every entry in a routing table in a single clock cycle.
			
			A second compression technique exploits the regular structure of the
			network's topology. If packets are due to travel along a straight line in
			the network without changing direction, routing entries are only required
			for routers where the packet enters the network, changes direction or is
			delivered to a core. Specifically, if a router receives a packet for which
			no routing entry matches, `default' routing is used where the packet is
			forwarded to the link opposite the one it entered. For example, a packet
			arriving on the North link would be forwarded to the South link. This
			technique makes it possible to construct routes where the number of
			routing entries required is $\le 1 + 2D$ where $D$ is the number of
			destinations, no matter how far apart the source and destinations of the
			route are.
			
			As a result of these compression techniques, SpiNNaker's routing tables
			contain 1,024 entries which is hoped to be sufficient for very large
			networks.
		
		\subsection{Link technologies}
			
			% Between and within chips use asynchronous links. Between boards this
			% would be impractical due to number of wires and performance over long
			% wires. Instead, high-speed serial via FPGAs is used. Minimal torus
			% construction.
			
			\begin{figure}
				\begin{subfigure}[b]{0.49\textwidth}
					\center
					\input{figures/threeboard}
					
					\caption{Tiling of three boards.\color{white}{\\~}}
					\label{fig:threeboard}
				\end{subfigure}
				\begin{subfigure}[b]{0.49\textwidth}
					\center
					\input{figures/threeboard-sliced}
					\vspace{0.5cm}
					
					\caption{Redrawn to show torus. Wrap-around connections connect
					opposing edges (not shown).}
					\label{fig:threeboard-sliced}
				\end{subfigure}
				
				\caption[Minimal tiling of three SpiNNaker forming a torus.]{Minimal
				tiling of three SpiNNaker forming a torus. Hexagons represent SpiNNaker
				chips on a board and the colour indicates which board a chip belongs
				to.}
				\label{fig:spinnaker-tiling}
			\end{figure}
			
			SpiNNaker chips are assembled on to boards of 48 chips. The chips on each
			board are logically connected into a hexagonal shape which allows boards
			to be tiled as shown in figure \ref{fig:threeboard}. Connections must then
			be made between chips on neighbouring boards to complete the network. The
			three boards shown can be further connected to form a torus, a process
			which is more clearly visualised in figure \ref{fig:threeboard-sliced}. By
			repeating this pattern, arbitrarily large torus networks can be
			constructed using triads of 48 chip boards.
			
			Between individual SpiNNaker chips, an asynchronous `2-of-7' protocol is
			used to communicate packets. This system enables low-power communication
			between chips which only uses energy during data transmission.
			Unfortunately, 2-of-7 codes use sixteen electrical signals (eight per
			direction) for each chip-to-chip link. Though this is acceptable for chips
			sharing a circuit board where wires are relatively cheap, connecting the
			768 wires to other boards is not practical. As a result, six bundles of
			eight chip-to-chip links each are multiplexed/demultiplexed onto six
			high-speed serial (HSS) links, one for each bundle. These links (described
			in greater detail in the next section) require only two signals but
			operate more than eight times faster than the 2-of-7 links and so no
			throughput penalty is incurred.
			
			The HSS links are implemented within the three FPGAs present on each 48
			chip SpiNNaker board. Each FPGA independently handles two of the HSS
			board-to-board links. These FPGAs also feature additional connectivity
			which may be exploited as part of the work proposed in this report.
	
	\section{High-Speed Serial (HSS)}
		\label{sec:high-speed-serial}
		
		% Technology now used by most supercomputers and high-speed comms tasks.
		% More complex than parallel but they scale up throughput better.
		
		\begin{figure}
			\begin{subfigure}{0.48\textwidth}
				\center
				\begin{tikzpicture}[thick, node distance=0.2cm, xscale=0.40]
					\input{figures/parallel-example}
					\input{|"python figures/parallel_example.py 'Hello, World!' 0.0"}
					
				\end{tikzpicture}
				
				\caption{No skew.}
				\label{fig:parcons-no-skew}
			\end{subfigure}
			\begin{subfigure}{0.48\textwidth}
				\center
				\begin{tikzpicture}[thick, node distance=0.2cm, xscale=0.40]
					\input{figures/parallel-example}
					\input{|"python figures/parallel_example.py 'Hello, World!' 0.7"}
				\end{tikzpicture}
				
				\caption{With skew.}
				\label{fig:parcons-with-skew}
			\end{subfigure}
			
			\caption[Parallel signalling example.]{Example of parallel data
			transmission. Eight bits are transferred each cycle and the value of the
			signals sampled at the point indicated by the faint line. As skew is
			introduced, errors appear.}
			\label{fig:parcons} \end{figure}
		
		Traditionally, digital links within computers have relied on parallel
		signalling where a `word' consisting of multiple bits is transmitted
		simultaneously each clock tick (figure \ref{fig:parcons-no-skew}). This
		mechanism is popular because it is simple and allows easy scaling by
		increasing the number of bits within in a word. Unfortunately, as the speed
		at which devices run increases, the assumption that the times taken by
		signals to propagate down different wires is the same has broken down. This
		`skew' between signals means that a receiver may erroneously receive bits
		from different words at the same time on different wires resulting in an
		error (figure \ref{fig:parcons-with-skew}). Though skew can be minimised by
		matching the quality and lengths of the wires used, this is expensive and
		does not scale well.
		
		An alternative parallel approach is delay-insensitive $M$-of-$N$ codes where
		a change in exactly $M$ out of a set of $N$ signals indicates a specific
		value \cite{bainbridge03}. For example, in SpiNNaker, the 2-of-7
		chip-to-chip links transmit four bits every time exactly two of the seven
		signals toggles.  Unfortunately, this approach requires that every value be
		acknowledged meaning a round-trip delay is incurred for every signal,
		severely limiting bandwidth.
		
		Serial signalling, by contrast, only transmits one bit at a time thus
		avoiding skew related issues. In addition, serial signalling reduces the
		number of wires required to transmit data. Since the number of
		connections required by a chip or circuit board can have a large influence
		on cost, this factor alone may make serial a necessary choice.
		
		While low-speed serial signalling techniques such as RS232 can be extremely
		simple, at higher speeds matters become more complex. This additional
		complexity has ramifications for applications of HSS and influence the work
		proposed later in this report. This section examines a number of the
		techniques employed by high-speed serial links and their implications for
		interconnection networks. A more in-depth treatment of HSS technology can be
		found by Athavale \emph{et. al.} \cite{athavale05}.
		
		\subsection{Clock recovery}
			
			% Requires frequent transitions (e.g. using 8b/10b) even on idle line,
			% makes idling expensive.
			
			Since serial signals can only transmit a single bit per clock cycle, to
			achieve high bandwidths HSS links typically operate at multiple gigahertz.
			Transmitting this clock signal alongside the data would be challenging to
			implement due to the need to minimise skew between the data and clock
			signals. Further, transmitting the clock would double energy required to
			drive the link. This is because energy is consumed when the signal along a
			wire is changed and, the longer the wire, the larger the amount of energy
			per transition. Since HSS links typically connect devices long distances
			apart, the energy requirements of just driving the signals can outweigh
			the consumption of supporting logic.
			
			An alternative to transmitting the clock signal is to extract the clock
			from the data stream itself. This is typically done using a Clock Recovery
			Circuit (CRC). This circuit can derive a clock signal from an incoming
			data stream if the data features frequent enough transitions between 0 and
			1. To guarantee this, data sent down the link must be encoded using a
			scheme such as 8b/10b coding \cite{widmer83} which translates arbitrary
			eight bit values and encodes these at ten bit sequences with guaranteed
			frequent transitions.
			
			The need for frequent transitions also implies that the link must
			constantly transmit values for the clock to be reconstructed by the
			receiver. This means that a lightly-loaded link consumes as much energy as
			a fully-loaded link. This is in sharp contrast with links using
			asynchronous $M$-of-$N$ codes which only draw power when data is
			transmitted.
			
			To reduce power consumption, HSS links typically have the ability to vary
			the link speed in response to load.  This requires the CRC to re-lock onto
			the clock used every time the link speed is changed. The time taken for
			the CRC to re-lock can cause serious problems for applications requiring
			low latencies since data cannot be transmitted during CRC re-lock. For
			realtime simulations of spiking neural networks delays can violate
			deadlines for message transmission.
		
		\subsection{Clock correction}
			
			High-speed serial links are typically designed to allow communication
			between two devices each using their own independent clock source to
			transmit data. This causes problems since no two clocks run at exactly the
			same speed meaning that a faster device will send slightly more data than
			the slower device can handle. For a clock with with an error of 100 Parts
			Per Million (PPM), this means that, in the worst case, in the time it
			takes a fast device to transmit 10,001 bits, a slow device would only be
			able to process 9,999 bits.
			
			Though a receiver is able to derive the clock used by the sender, this
			clock is typically only used to place the data from the incoming serial
			steam into a FIFO buffer. Data placed in this buffer is then read by the
			receiving system using its own clock. Unfortunately, this buffer will
			inevitably overflow or underflow depending on the relative speed of the
			sender's clock. One of the additional features provided by 8b/10b coding
			is the provision of a number `out-of-band' codes, known as K-characters,
			which do not correspond to any eight bit sequence. To avoid buffer
			under/overflow, senders periodically insert a `skip sequence' of a number
			of K-characters.  If the receiver's clock is running slower than the
			sender's, its buffer will gradually fill up. If the buffer fills beyond a
			high-water mark, the receiver will drop any skip sequences it receives
			allowing it to catch up and the buffer to empty. Conversely, if the
			receiver is faster than the sender, its buffer will gradually empty.  If a
			low-water mark is passed, the receiver may duplicate any skip sequences in
			its buffer.
			
			Clock correction hardware therefore adds a certain amount of buffering
			which contributes both to chip area requirements and more importantly
			increases the latency suffered when traversing a HSS link. This latency
			cost can be an important factor for realtime neural simulators such as
			SpiNNaker where data must arrive at its destination in the network within
			a short time window.
		
		\subsection{Error recovery}
			
			No link technology is perfectly reliable with HSS being no exception and
			thus is susceptible to occasional bit errors. As a result, an error
			detection and recovery mechanism must be used to ensure reliable delivery
			of data.
			
			In noisy communication systems such as wireless networks, where bit errors
			are frequent, Forward Error Correction (FEC) is used which redundantly
			encodes data such that it is still recoverable in the presence of certain
			errors \cite{hamming50}. HSS links are typically far less noisy and thus
			errors are comparatively rare.  To avoid the significant communication and
			computation overhead involved in FEC, HSS links typically use checksums to
			detect the validity of arriving data and a resend mechanism to request
			retransmission when corrupted data arrives.
			
			Resend mechanisms require that senders retain copies of data due to be
			sent over a link until it is acknowledged as successfully received. The
			number of packets which must be retained is determined by the frequency
			with which acknowledgements are sent for correctly arriving data. The
			overheads introduced by transmitting acknowledgements along with the
			buffering used are parameters that must be tuned to for a given
			application. These overheads typically are far less significant than those
			added by FEC when the number of errors is low.
			
			Retransmission does, however, add significantly to the latency cost of
			errors since a round-trip delay is incurred in the sending of a negative
			acknowledgement and awaiting retransmission. Once again, this latency can
			be significant for realtime systems.
		
		\subsection{Flow control}
			
			HSS links exhibit relatively high latencies due to the buffering required
			by clock correction and 8b/10b coding. As a result of this, flow control
			cannot be a simple handshaking mechanism where blocks of data are
			individually acknowledged since waiting for these signals would introduce
			a large overheads. Instead, more sophisticated schemes such as credit
			based flow control \cite{dally04} are be used.  These mechanisms introduce
			further complexity and buffering but typically only a minimal latency
			penalty.
