\chapter{Research plan}
	\label{sec:research-plan}
	
	\begin{figure}[b!]
		\center
		\input{figures/plan-gantt}
		\caption[Gantt chart overview of research plan.]{Gantt chart overview of
		research plan. Boxes indicate expected duration of a task, thick lines
		indicate slack and red arrows show dependencies between tasks. Note the
		non-linear scale.}
		\label{fig:plan-gantt}
	\end{figure}
	
	The preliminary work so-far has focused on the analysis and exploration of
	extensions to SpiNNaker's interconnection network. In particular, it focuses
	on the high-speed serial links which connect chips on different boards within
	the system. The proposed research plan is summarised in figure
	\ref{fig:plan-gantt} and aims to continues this work.
	
	The proposed work can be divided into two distinct parts. The first
	investigates the use of small-world, or small-world-like networks within
	actual SpiNNaker systems. The second extends this to attempt to improve power
	efficiency within SpiNNaker by powering-off or slowing down links where
	lightly-loaded alternatives exist.
	
	To measure success, realistic network traffic from neural simulations, along
	with the collection of suitable metrics will be required. Building a
	representative selection of benchmark networks, along with the infrastructure
	for assessing their impact on SpiNNaker's interconnect will be a key part of
	the work undertaken.
	
	This chapter will outline each of the key research aims and the work proposed
	in approximately chronological order.
	
	\section{Nengo network experiments}
		
		As described earlier, the NEF provide an actively used neural modelling
		architecture capable of generating large scale neural models. Unlike many
		other frameworks the NEF is already being used to construct large, models
		from simple neurons, the design target of SpiNNaker. The SPAUN model, for
		example, contains 2.5 million simple neurons \cite{eliasmith12}, while
		models produced by students at the two-week Nengo 2014 Summer School
		constructed networks containing almost half a million neurons, even within
		this short timespan. As well as being large, Nengo models generally feature
		easily verified high level-behaviour. This feature enables easier
		identification of system failures and can help guide efforts to debug
		issues.
		
		In addition to the above, Nengo's SpiNNaker implementation lends itself to
		convenient analysis due to its unconventional implementation. Though still
		simulating spiking neurons, the spikes themselves are not transmitted over
		SpiNNaker's network, instead, the value represented by the spikes is
		transmitted. This approach exploits the observation that a population with
		hundreds of neurons will typically represent a vector of at most low tens of
		values. Though the value must be transmitted every time step (typically 1ms)
		and requires a `long' 72-bit SpiNNaker packet for each element, a large
		population of neurons in Nengo typically produces a large number of spikes
		each time step, each requiring a `short' 40 bit packet. For example, for a
		population of 1,000 neurons representing a two-valued vector and firing at
		an average rate of 10 Hz would produce around ten 40-bit packets (400 bits)
		to be transmitted each millisecond.  Alternatively, if the represented value
		was transmitted, only two 72-bit packets would be required (144 bits). In
		reality, many Nengo models feature firing rates far higher than the 10 Hz
		average found in the brain and as a result, this mechanism can result in
		significant savings in network resources.
		
		Transmission of represented value has the side-effect of making the overall
		rate of packet transmission deterministic: the same number of values must be
		transmitted every millisecond, regardless of the simulated network's state.
		This means that artificially reproducing the traffic patterns and modelling
		them analytically can be greatly simplified.
		
		Initially, work will be carried out to analyse the effects of traffic
		patterns produced by large networks in SpiNNaker. Nengo allows users to
		change the underlying neuron model. This mechanism will be used to define a
		new `neuron type' which instead of actually modelling a neuron simply
		generates an equivalent traffic pattern and use spare resources to records
		network behaviour. Since most Nengo networks use a single neuron type, this
		means only a single parameter change is required to analyse a given network.
		Additionally, this approach allows great flexibility in the metrics recorded
		since cores will be largely idle.
		
		As well as `real' Nengo models, a number of synthetic models designed to
		stress network resources will also be created. These will be useful for
		directly determining classes of challenging traffic patterns.
		
		This work is expected to produce two major outcomes: an improvement in
		Nengo's performance on SpiNNaker and also the creation of a suite of
		benchmarks based on real Nengo networks. The former improvement is expected
		to result primarily from a better understanding of the effects that
		different patterns of packet transmission have on network performance. Some
		improvement is also anticipated through changes to the placement and routing
		algorithms used for allocating groups of neurons to cores in the system.
		This work is planned for journal publication by the end of the acedemic year
		though a specific journal has not yet been selected.
		
	
	\section{Small-world networks for SpiNNaker}
		
		% Develop routing scheme for SpiNNaker packets suitable for peripheral, host
		% and preliminary small-world communications via the SpiNNaker links. This
		% scheme will continue to make use of the current HSS protocol blocks.
		
		In order to exploit the benefits of small-world network topologies, work
		will be carried out to attempt to implement them within SpiNNaker's
		board-to-board interconnection network. Work will begin by extending the
		Tickysim network simulator with small world connectivity and a model of
		SpiNNaker's board-to-board links. Guided by these simulation results, this
		will be followed by an implementation in SpiNNaker.  This implementation
		will then be evaluated using the Nengo benchmarks described previously.
		
		There are three main challenges associated with an implementation of
		small-world networks in SpiNNaker which are outlined in this section. The
		first is the construction of a routing scheme which can be practically
		implemented within the available FPGA logic. The second is deciding the
		method by which random links are added and which effect this has on physical
		assembly difficulties and performance. The final challenge, which will be
		undertaken only if time allows, is the possibility of routing packets
		between FPGAs on the same board. This would allow, for example, packets to
		`hop over' an entire board instead of traversing the every chip along the
		path through the board.
		
		\subsection{Routing}
			
			One of the principle challenges of implementing new network topologies
			within the FPGAs on a SpiNNaker board is that all routing must take place
			within the FPGA itself. Within a SpiNNaker chip, routing is handled by
			dedicated routing hardware featuring a generous ternary CAM which may
			prove expensive within FPGA logic \cite{locke11}. Additionally, each FPGA is
			responsible for sixteen individual streams of traffic, all of which must
			be routed without a reduction in throughput.
			
			In preliminary work, a very simple single-entry key and mask routing
			scheme proved adequate for simple host connectivity.  This scheme consists
			of a mask and comparison performed on each packet to determine its route
			which can be implemented very cheaply within the FPGA logic.  This
			approach is adequate here because generally there will be only a few I/O
			devices in a system compared with the number of neurons and so it is
			feasible to allocate a bit out of all routing keys to indicate such
			packets.  For small world networks, however, allocating a bit in the key
			space to indicate when each small-world link should be used would require
			more bits than available in routing keys due to the greater number of
			small-world links.
			
			One possible solution to this problem is to exploit the relative speed
			differences between the FPGA logic and the links to SpiNNaker chips. A
			SpiNNaker chip link can nominally transmit a (short) packet in around 167
			$\mu$s while FPGA logic clocked at 75 MHz cycles in around 13 ns. This
			means that if a router with a throughput of one packet per cycle was
			implemented, up to twelve links could be handled by a single router
			without impacting on throughput. This means a that only two such routers
			would be required to handle all sixteen of each FPGA's links. A
			feasibility study will be required to establish the potential capability
			of such routers given the resources available on the FPGAs on each board.
			
			An alternative approach is to allow the FPGAs to modify the routing keys
			of packets which pass through them. If it is found that most packets will
			only use a single small-world link, a single bit could be defined within
			the key space to indicate that a packet should use the first small-world
			link it encounters. This bit would be cleared as it crosses the small
			world link after which the packet would be routed normally. This method
			would be able to exploit the existing key and mask routing scheme already
			implemented at the expense of greater complexity in the placement and
			routing system.
			
			As well as the technological factors described, the choice may be guided
			by time constraints since a basic implementation of the second approach is
			likely to be significantly less time consuming than a full FPGA based
			router implementation.
		
		\subsection{Random connectivity}
			
			The method by which random connectivity is introduced into small-world
			networks can have a significant impact on both the amount of manual labour
			required to install it and also the actual performance gains achieved. One
			approach is to generate random connections and produce a list of wiring
			instructions for a technician to assemble. An alternative is to allow the
			technician to add the connections randomly at their own discretion.
			
			Computer generated random connections have the advantage that they can be
			both unbiased and also may be post processed to prune non-beneficial links
			which, for example, happen to connect two already neighbouring or
			nearly-neighbouring boards. The main downside of this approach is that the
			effort required to manually assemble a long list of connections may be
			great, especially for large machines where there may be as many as 3,600
			potential connection points.
			
			The alternative approach, while straightforward for a technician's wiring
			will suffer from the poor quality of human-generated randomness
			\cite{figurska08}. Additionally, humans are unlikely to be able to spot
			and avoid `obviously poor' choices due to the complexity of the wiring in
			large systems.
			
			Some work is required to establish the efficacy of either approach. Simple
			experiments building on the simple graph based models used in preliminary
			work will be carried out to attempt to judge the importance of the
			randomness of decision making.
		
		\subsection{Board-hopping}
			
			% TODO: Diagram?
			
			The FPGAs on SpiNNaker boards are interconnected allowing them to
			communicate locally via a HSS ring network. If time allows, this could be
			used to directly forward packets which would otherwise simply pass
			directly through all the chips the board before emerging on the other
			side. This mechanism will reduce contention for link resources on
			intervening chips while potentially reducing the latency for the packet
			forwarded. This mechanism is a variant of an express cube \cite{dally91}
			which exploits existing unused hierarchy in the network.
		
		\subsection{Evaluation}
			
			The suite of Nengo benchmark networks described in the previous section
			will be used to evaluate the performance of the system. It is hypothesised
			that large, heavily connected networks will benefit from the additional
			small-world connectivity since these are where longer connections are
			likely to exist as well as greater contention for link resources.
	
	
	\section{USB 3.0 interface}
		
		As part of the collaboration started at the Capo Caccia workshop to develop
		a high speed interface for communications with SpiNNaker a USB 3.0 system
		will be produced. This work is awaiting the completion of a circuit board
		design on which the interface will be implemented. No firm date has been
		given for arrival of a prototype board and so the dates indicated in figure
		\ref{fig:plan-gantt} are approximate. This work will consist of an extension
		of the UART proof-of-concept FPGA design which interfaces with a high-speed
		Cypress FX-3 USB 3.0 interface chip in place of the low-speed FTDI
		UART-over-USB interface chip.
		
		Given the timely availability of this device, it will allow superior
		monitoring of the SpiNNaker network since packets travelling over a link can
		be `snooped' via the USB link without impacting the system's performance.
		Additionally, it would allow software models of high-bandwidth I/O devices
		and host communications to be implemented.
	
	
	\section{High-speed serial power management}
		
		% Work to try and make some power savings by powering down idle links or
		% reducing speed (to reduce link transitions), taking into account latency
		% concerns for SpiNNaker.
		
		Power consumption is an important consideration for large SpiNNaker
		machines. At present, SpiNNaker's high-speed serial links operate at full
		speed at all times, even when no traffic is present, potentially wasting a
		significant amount of energy. While numerous techniques for power managing
		HSS technologies exist, these tend to be optimised toward more conventional
		super computer loads.
		
		In general, power savings are achieved either by powering down idle or
		underutilised links or by running them at low speeds. These techniques
		introduce problems for latency-sensitive, lightly-loaded interconnection
		networks such as SpiNNaker. Powering up or changing link speeds introduces
		an inherent delay since it requires the sender and receiver's CRC to re-lock
		on to the new frequency which can take on the order of a millisecond
		\cite{xilinx14}. Though this latency may be acceptable for some systems, it
		would violate the SpiNNaker realtime requirement that packets are delivered
		within 1 ms.
		
		This section describes two complementary proposals for exploiting these
		existing low-power techniques while also maintaining realtime performance.
		The section concludes with a possible collaboration with the Technical
		University of Dresden where a link with better power-up or speed-change
		performance may be developed.
		
		\subsection{Link oversampling}
			
			% During PLL locking, could still have good performance
			
			During clock recovery, the clock signal it produces is of low quality but
			gradually approaches the ideal signal. It is hypothesised is that during
			this acquisition period, data may still be sent, but at low speeds.  This
			could be achieved, for example, by duplicating every bit sent across the
			link (oversampling) and thus reducing sensitivity to the poor quality
			clock signal.  Further error detection information may also be included
			during this time to handle the increased error rate. This technique could
			reduce the latency caused by link speed changes by allowing useful data to
			pass across links earlier.
			
			A large part of this work will consist of characterising the quality of
			the link during clock recovery and thus determining the degree of
			oversampling required.
		
		\subsection{Traffic redirection}
			
			% Send traffic around other edges of a triangle
			
			\begin{figure}
				\center
				\input{figures/emergency-routing}
				
				\caption[Emergency routing example.]{Emergency routing example. A
				faulty link is avoided by traversing two neighbouring links.}
				\label{fig:emergency-routing}
			\end{figure}
			
			An alternative to attempting to use links during clock recovery is to
			redirect traffic via an alternative route. One of the advantages of the
			hexagonal torus topology is a high level of redundancy in routing
			possibilities.  At the chip-to-chip level, SpiNNaker exploits this in its
			`emergency routing' scheme where packets are automatically re-routed
			around faulty links as shown in figure \ref{fig:emergency-routing}. These
			emergency routes feature only a single hop of overhead and additionally
			require only very simple, fixed routing rules. Since the HSS
			board-to-board links within SpiNNaker are also arranged in a (coarser)
			hexagonal torus, the same principle can in be used here.
			
			\begin{figure}
				\center
				\input{figures/board-to-board-redirection}
				
				\caption[Board-to-board traffic redirection.]{Board-to-board traffic
				redirection. An unavailable link is avoided (e.g. when faulty, during
				initial clock recovery or powered down) by traversing two neighbouring
				HSS links. Each group of small hexagons (SpiNNaker chips) represents a
				board. Rectangular blocks represent board-to-board HSS blocks. Dotted
				lines indicate HSS blocks which share an FPGA and thus may communicate
				cheaply. Lines between boards are HSS links.}
				\label{fig:board-to-board-redirection}
				\end{figure}
			
			Figure \ref{fig:board-to-board-redirection} illustrates how frames
			destined for an unavailable HSS link are instead transmitted via a
			neighbouring link.  Though this alternative route appears much longer,
			only two HSS links are crossed (an overhead of one HSS hop). Since each
			FPGA implements two HSS board-to-board links, frames destined for an
			unavailable link can be cheaply forwarded within the FPGA to the other
			link for transmission. The intermediate FPGA then blindly forwards the
			frame over its other HSS link where the frame finally arrives at the
			intended FPGA. The arriving frame is then processed as if it had arrived
			directly.
			
			As with emergency routing, the redirection technique only works when at
			most one of the HSS links within a set of three is unavailable. As a
			result, link speed changes must be carefully scheduled in order to ensure
			that only one link within the set changes speed at a given moment.
		
		\subsection{`Santos-28' test chip}
			
			% A 28 nm test chip is being developed with TU Dresden to evaluate various
			% technologies' performance for inclusion in a second generation SpiNNaker
			% chip. There may be room in this space for my own involvement in this
			% work at short notice.
			
			In a collaboration between the University of Manchester and the Technical
			University of Dresden a test chip is being developed to pave the way
			towards the next generation of SpiNNaker system. The test chip will be
			manufactured using a modern 28 nm process and will notably make use of HSS
			links both internally and between chips. The proposed HSS links have been
			developed by the researchers in Dresden and therefore are potentially open
			to development. Due to the currently unpredictable state of this project,
			this work does not appear in the Gantt chart. Should the work take place,
			it will initially replace the other work planned in this section.
	
	\section{Thesis writing}
		
		A large block of time is reserved for the production of a thesis. This
		allocation ensures that adequate time is made available for the collection
		of additional data which are found to be missing at the time of writing.
