\chapter{Introduction}
	
	Modern computer systems are some of the most complex devices ever constructed.
	Current computer technologies have enabled everything from global,
	near-instantaneous communications via the internet to faster and more
	effective cancer treatments \cite{nassif}. Despite this, the brain still
	outperforms conventional digital computers at many tasks; for example brains
	are able learn far more flexibly than any known computer system.
	
	Considerable effort has been made by researchers to understand how the brain
	works. The small-scale operation of individual neurons in the brain is now
	relatively well understood but the manner of their complex interactions is
	still not clear. Current attempts to understand this involve modelling the
	behaviour of huge networks of neurons with the hope of gaining a better
	understanding of their collective behaviour. Such models are challenging to
	fit into modern computer architectures since the vast parallelism available to
	neurons in the brain sharply contrasts with the highly serial computing
	resources available today.

	In this report I will elaborate on the role that modelling has on
	understanding the brain and in particular on the work being done to build
	machines to fulfil this task. I will also outline the importance of the
	interconnection networks in brain simulations and describe the work I have
	been doing in this area.
	
	\section{Why model the brain?}
	
		Cutting-edge neural models can be broadly divided into two types. The first
		type uses simplified models of neurons in large networks. Models of this
		type, such as Spaun \cite{eliasmith12}, are able to demonstrate a remarkable
		range of cognitive abilities such as memory, problem solving and pattern
		recognition. Spaun is notable due to its range of functional and realistic
		behaviours. For example, it exhibits very similar behaviour to humans in
		terms of the way its behaviour degrades over time due to aging or illness.
		It also suffers the same gradual reduction in function as random neurons die
		off. This level of realism means that experiments which would not be
		possible to carry out on real brains, such as testing hypotheses of the
		causes of certain illnesses, may become feasible using simulated models.
		
		In contrast the second type of model has focused on developing
		high-accuracy simulations of much smaller collections of neurons to test
		understanding of biological processes. Such models are highly biologically
		plausible but do not result in the complex, high-level behaviour shown by
		simulators such as Spaun.
		
		This work focuses on the first type of model as these models present a
		greater challenge to modern computer systems. Models of the second type,
		such as the Blue Brain project, are able to utilise commercial
		supercomputers to achieve simulation speeds around one order of magnitude
		below biological real-time \cite{markram06}. By contrast models of the first
		type, such as Spaun, do not easily fit current supercomputer architectures
		and can take around two and a half hours of compute time to simulate one
		second of neural activity on a high-end workstation computer.
		\S\ref{sec:simulating-brains} describes the basic mechanism of these neural
		simulators and their computational challenges in further detail.
	
	\section{Conventional supercomputer approaches}
	
		Conventional supercomputers are designed to provide immense computational
		power with quadrillions ($10^{15}$) of numerical calculations being
		performed per second. Despite such impressive feats of computation, these
		machines often feature relatively slow interconnection networks tying the
		internal processing elements together. This balance of great computation and
		limited communication is contrary to the brain whose individual neurons
		offer relatively little computational power but are extraordinarily well
		connected.
		
		The Blue Brain project is unusual in its use of conventional supercomputers
		but is severely limited in network size because of this.  Only around
		100,000 neurons can be simulated compared with almost 100 billion in the
		human brain.
		
		\S\ref{sec:supercomputers} goes into greater detail on current super
		computer architectures and their strengths and weaknesses.
	
	\section{The importance of interconnect}
		
		Given the unsuitability of traditional supercomputer architectures, my
		research focuses on the challenge of developing new architectures for the
		simulation of large-scale networks, such as Spaun, which are heavily
		communication bound.  The SpiNNaker project \cite{furber06} has developed
		one such architecture based on a network of over one million small,
		low-power processors. The purpose-built interconnection network is designed
		to handle neural simulations with up to one billion neurons running in
		biological real-time. A detailed introduction to the SpiNNaker architecture
		is given in \S\ref{sec:spinnaker}.
		
		% TODO: reference relevant section where I exploit HSS.
		
		SpiNNaker's interconnection network is designed to allow efficient
		transmission of signals produced by simulated neurons. As the system scales
		up from tens to tens-of-thousands of nodes, new types of high-speed serial
		(HSS) based interconnection technology have been introduced to keep the
		wiring in the system practical. These technologies, described in
		\S\ref{sec:high-speed-serial}, have very different qualities to SpiNNaker's
		original network and my work aims to better understand and exploit this
		resource.
		
		Practical issues when designing interconnect, such as wiring complexity, are
		studied in \S\ref{sec:wiring-up-large-spinnaker-machines} where the task of
		wiring up large SpiNNaker systems consisting of 1,200 circuit boards is
		tackled. This is followed by the development of an unconventional,
		semi-random interconnection network design which takes into account these
		wiring considerations in \S\ref{sec:small-world-supercomputers}. This
		semi-random design of network has the potential to improve the performance
		of SpiNNaker with a minimal additional cost in cabling requirements.
	
		My work will develop a new architecture intended to succeed SpiNNaker with a
		focus on the design of the interconnection network.
		\S\ref{sec:research-plan} outlines the research I plan to undertake to reach
		this goal following on from my preliminary studies.
