\chapter{Introduction}
	
	Modern computer systems are some of the most complex devices ever constructed.
	Current computer technologies have enabled everything from global,
	near-instantaneous communications via the internet to faster and more
	effective cancer treatments \cite{nassif}. Despite this, the brain still
	outperforms conventional digital computers at many tasks such as learning,
	fault tolerance and energy efficiency.
	
	Considerable effort is being made by researchers across many disciplines to
	understand how the brain works. The small-scale, chemical operation of
	individual neurons in the brain is now relatively well understood but their
	complex interactions and behaviour remain murky \cite{dayan03}. Many current
	attempts to understand the brain's higher-level behaviour involve modelling
	huge networks of neurons to observe their collective behaviour.  Such models
	are challenging to fit into modern computer architectures since the vast
	parallelism and interconnection resources available to neurons in the brain
	sharply contrasts with the highly serial, comparatively isolated computing
	resources available today.
	
	This report will elaborate on the role of modelling in understanding the brain
	and, in particular, on the work being done to build specialist machines to
	fulfil this task. In particular it will focus on the interconnection networks
	of these devices and my work in this area.
	
	\section{Why model the brain?}
	
		Cutting-edge neural models can be broadly divided into two categories. The
		first focuses on large networks of simplified models of neurons.  The second
		instead focuses on very small numbers of extremely detailed neuron models.
		
		Models of this type, such as Spaun \cite{eliasmith12}, are able to
		demonstrate a remarkable range of cognitive abilities such as memory,
		problem solving and pattern recognition. Models such as Spaun are notable
		due to their range of functional and realistic behaviours. For example,
		Spaun exhibits very similar responses to humans when faced with simulated
		ageing processes and illness. In particular, it also suffers the same
		gradual reduction in overall function as individual neurons die off, a
		property not shared by modern computers which can fail immediately with the
		loss of a single transistor. This level of realism could soon enable
		experiments which would not be possible to carry out on real brains, such as
		testing hypotheses of the causes of certain illnesses.
		
		In contrast the second type of model has lead to a much deeper understanding
		of biochemical processes. Though these models tend to feature great
		biological accuracy, they typically offer only limited into the larger-scale
		function of the brain.
		
		As a computer scientist, the first type of model presents two particularly
		enticing properties aside from the enhancement of biological understanding.
		The first is their growing capability for fault-tolerant, efficient
		high-level computation from which much could be learnt. The second property,
		and the focus of my research, is that the large-scale models present a huge
		challenge to modern computing systems due to their sheer scale and unusual
		computational requirements. Models of the second type, such as the Blue
		Brain project, are able to utilise commercial supercomputers to achieve
		simulation speeds only one order of magnitude below biological real-time
		\cite{markram06}. By contrast models of the first type, such as Spaun, do
		not easily fit current supercomputer architectures and can take around two
		and a half hours of compute time to simulate one second of neural activity
		on a high-end workstation computer. In \S\ref{sec:simulating-brains} the
		basic mechanism of large-scale neural models is described along with the
		associated computational challenges in further detail.
	
	\section{Conventional supercomputer approaches}
	
		Supercomputers such as Tianhe-2, currently the worlds
		fastest\cite{meuer13n}, are designed to provide immense computational power
		with quadrillions ($10^{15}$) of numerical calculations being performed per
		second. Despite such impressive feats of computation, these machines often
		feature interconnection networks, which tie the internal processing elements
		together, with comparatively low performance. This balance of great
		computation and modest communication is contrary to the brain whose
		individual neurons offer relatively little computational power but are
		extraordinarily highly interconnected.
		
		This mismatch can be seen in the simulation performance figures posted by
		leading large scale experiments. For example IBM, as part of DARPA's SyNAPSE
		programme, have constructed a model featuring $53 \times 10^{10}$ neurons
		connected via $1.37 \times 10^{14}$ synapses on a Blue Gene/Q super computer
		\cite{ibm13}. Though a formidable achievement, the model runs $1,542\times$
		slower than biological real-time and consumes $394,500\times$ more power
		than a similarly sized brain \cite{drubach00}. In the short term, this
		prevents the use of such models in robotics experiments which require
		real-time performance. In the longer term, realistic models which take years
		to develop would find themselves requiring multi-millennia runtimes just to
		reach the level of a toddler rendering such work impractical.
		
		\S\ref{sec:supercomputers} goes into greater detail on the current
		state-of-the-art for super computer architectures and their suitability for
		neural simulations.
	
	\section{The importance of interconnect}
		
		Given the unsuitability of traditional supercomputer architectures, a number
		of specialised architectures have been developed. These architectures
		generally feature novel computational resources and, importantly, generous
		interconnection networks.
		
		The SpiNNaker project \cite{furber06} is one such architecture based on a
		custom interconnection network which is being scaled to over one million
		small, low-power computer processors. The purpose-built interconnection
		network is designed to handle the unusual traffic load of neural simulations
		with up to one billion neurons and one trillion synapses running in
		biological real-time.
		
		As SpiNNaker scales up from hundreds of processors to millions, a new type
		of `high-speed serial' (HSS) interconnect technology has been introduced to
		keep the construction of the machine practical. These technologies have very
		different qualities to SpiNNaker's original network and my research aims to
		better understand and exploit this new resource. In \S\ref{sec:spinnaker}
		the SpiNNaker architecture is introduced in greater detail and in
		\S\ref{sec:high-speed-serial} the properties of the new link technologies
		being introduced are described.
		
		
		% TODO: reference relevant section where I exploit HSS.
		
		Preliminary work, described in chapter \ref{sec:preliminary-work} has
		focused on building better understanding of SpiNNaker's native interconnect
		and investigating ways of better exploiting the resources of the new HSS
		interconnect.
	
		From this foundation, my research will develop improved techniques for
		utilising HSS in the context of neural simulation systems such as SpiNNaker
		with a focus on energy efficiency and simulation speed. Chapter
		\ref{sec:research-plan} outlines the research plan that will be undertaken
		to reach this goal.
