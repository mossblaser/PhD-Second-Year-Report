\chapter{Introduction}
	
	Modern computer systems are some of the most complex devices ever constructed.
	Current computer technologies have enabled everything from global,
	near-instantaneous communications via the internet to faster and more
	effective cancer treatments \cite{nassif}. Despite this, the brain still
	outperforms conventional digital computers at many tasks such as learning,
	fault tolerance and energy efficiency.
	
	Considerable effort is being made by researchers across many disciplines to
	understand how the brain works. The small-scale, chemical operation of
	individual neurons in the brain is now relatively well understood but their
	complex interactions and emergent behaviours remain murky \cite{dayan03}. Many
	to understand the brain's higher-level behaviour involve modelling huge
	networks of neurons to observe their collective behaviour.  Such models are
	challenging to simulate in traditional computer architectures since the vast
	parallelism and interconnection exhibited by neurons in the brain sharply
	contrasts with the highly serial, comparatively isolated computing resources
	available today.
	
	This report examines the challenges encountered when modelling the brain,
	forcusing primarily on those faced by computer interconneciton networks.
	Earlier parts of the report focus on the techniques and impact of current
	modelling techniques. This is followed by a background material describing the
	attempts being made to improve simulation performance and the importance of
	interconnect. The later parts of the report outline the work being done to
	advance the state-of-the-art of neural simulator interconnect followed by
	plans for the continuation of this work.
	
	\section{Why model the brain?}
	
		Cutting-edge neural models can be broadly divided into two categories. The
		first focuses on the behaviour of large networks of simplified models of
		neurons.  The second instead focuses on very small numbers of extremely
		detailed neuron models.  Models of the former type, such as Spaun
		\cite{eliasmith12}, are able to demonstrate a remarkable range of cognitive
		abilities such as memory, problem solving and pattern recognition. Unlike
		non-neural models such as ACT-R \cite{anderson93}, Spaun exhibits very
		similar responses to humans when faced with simulated ageing processes and
		illness. This level of realism could soon enable experiments which would not
		be possible to carry out on real brains, such as testing hypotheses of the
		causes of certain illnesses and possible treatments.
		
		% It also shows the same gradual reduction in overall function as individual
		% neurons die off, in sharp contrast with modern computers which typically
		% fail immediately with the loss of a single transistor. 
		
		In contrast the later type of model has lead to a much deeper understanding
		of biochemical processes in the brain. Though these models tend to feature
		great biological accuracy, they typically offer only limited insight into
		the larger-scale function of the brain.
		
		To computer scientists, the first type of model presents two particularly
		enticing properties, aside from the enhancement of biological understanding.
		The first is neural systems' exhibition of fault-tolerant, efficient
		high-level computation from which much could be learnt in the field of
		computer design. The second property, and the focus of this work, is that
		large-scale models present a huge challenge to modern computing systems due
		to their sheer scale and heavy communication requirements. Models such as
		Spaun, do not easily fit current supercomputer architectures and can take
		many hours of compute time to simulate one second of neural activity on a
		high-end workstation computer. This limited performance is stifling research
		in this field, highlighting the importance of new, faster approaches to
		neural simulation.
	
	\section{Conventional supercomputer approaches}
	
		Supercomputers such as Tianhe-2, currently the worlds
		fastest\cite{meuer13n}, are designed to provide immense computational power
		with quadrillions ($10^{15}$) of numerical calculations being performed per
		second. Despite such impressive feats of computation, these machines often
		feature interconnection networks with comparatively low performance, tying
		internal processing elements together. This balance of great computation and
		modest communication contrasts with the brain whose individual neurons offer
		relatively little computational power but are highly interconnected.
		
		This mismatch is evident in the simulation performance figures posted by
		leading large scale experiments. For example IBM, as part of DARPA's SyNAPSE
		programme, have constructed a model featuring $53 \times 10^{10}$ neurons
		connected via $1.37 \times 10^{14}$ synapses on a Blue Gene/Q super computer
		\cite{ibm13}. Though a formidable achievement, the model runs $1,542\times$
		slower than biological real-time and consumes $394,500\times$ more power
		than a similarly sized brain \cite{drubach00}. In the short term, this
		prevents the use of such models in robotics experiments which require
		real-time performance. In the longer term, developmental models would find
		themselves requiring multi-millennia runtimes just to reach the level of a
		toddler rendering such work impractical.
		
	\section{The importance of interconnect}
		
		Given the unsuitability of traditional supercomputer architectures, a number
		of specialised architectures have been developed. These architectures
		generally feature novel computational resources and, importantly, generous
		interconnection networks designed for neural simulation loads.
		
		The SpiNNaker project \cite{furber06} is one such architecture based on a
		custom interconnection network which is being scaled to over one million
		small, low-power computer processors. The purpose-built interconnection
		network is designed to handle neural simulations with up to one billion
		neurons and a total of one trillion neural connections (synapses) running in
		biological real-time.
		
		As the SpiNNaker machine is scaled up from hundreds of processors to
		millions, a new type of interconnect technology (`high-speed serial' (HSS))
		has been introduced to keep the construction of the machine practical. This
		technology has very different qualities to SpiNNaker's original network and
		this research project aims to better understand and exploit this new
		resource.
		
		\section{Report structure}
			
			The report begins in Chapter \ref{sec:background} by introducing the
			nature of neural modelling tools. This is followed by a discussion of the
			challenges faced by conventional super-computer architectures in
			simulating these models. The chapter concludes by describing the SpiNNaker
			neural simulator, and the interconnection network technology it uses,
			which serves as a platform for this work.
			
			In chapter \ref{sec:preliminary-work} preliminary work is described to
			model, explore and propose extensions to SpiNNaker's interconnect. This
			work examines the physical wiring required by large simulation systems and
			proposes a novel extension to this wiring scheme turning the system into a
			`small-world' network with a number of attractive properties. Finally,
			foundational work on SpiNNaker's interconnect is described to allow both
			the implementation of the small-world wiring scheme and also offer
			high-speed connectivity between a SpiNNaker machine and the outside world.
			
			Continued work on SpiNNaker's interconnect is proposed in chapter
			\ref{sec:research-plan}. This work includes the development of a benchmark
			suite based on a combination of real neural networks and synthetic tests.
			Based on these benchmarks, work will progress with an implementation of
			small-world wiring in SpiNNaker. This is followed by work to construct a
			novel mechanism to allow SpiNNaker's interconnect to exploit power-saving
			strategies which currently would fatally impact SpiNNaker's real-time
			performance target.
